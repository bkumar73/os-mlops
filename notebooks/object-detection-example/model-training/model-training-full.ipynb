{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d3cb1-bd2a-47a5-a750-8bfc32d3a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from math import floor\n",
    "from os import environ, listdir, makedirs, path, unlink\n",
    "from shutil import copy, move, rmtree\n",
    "import yaml\n",
    "\n",
    "from boto3 import client\n",
    "from numpy import random\n",
    "from openimages.download import download_dataset\n",
    "from yolov5 import export, train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196667f-a9e1-46d2-820b-8a924a5cad48",
   "metadata": {},
   "source": [
    "## Step 1: ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d15faa-2d60-4840-aa5f-621715bddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(\n",
    "        data_folder='./data', limit=0,\n",
    "        configuration_path='configuration-local.yaml'):\n",
    "    _clean_folder(data_folder)\n",
    "    class_labels = _read_class_labels(configuration_path)\n",
    "\n",
    "    print('Commencing data ingestion.')\n",
    "\n",
    "    limit = limit or int(environ.get('sample_count', 100))\n",
    "    download_folder = f'{data_folder}/download'\n",
    "\n",
    "    download_dataset(\n",
    "        download_folder,\n",
    "        class_labels=class_labels,\n",
    "        annotation_format='darknet',\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    print('data ingestion done')\n",
    "\n",
    "\n",
    "def _clean_folder(folder):\n",
    "    print(f'Cleaning folder {folder}')\n",
    "\n",
    "    for filename in listdir(folder):\n",
    "        file_path = path.join(folder, filename)\n",
    "        try:\n",
    "            if path.isfile(file_path) or path.islink(file_path):\n",
    "                unlink(file_path)\n",
    "            elif path.isdir(file_path):\n",
    "                rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "\n",
    "def _read_class_labels(configuration_file_path):\n",
    "    with open(configuration_file_path, 'r') as config_file:\n",
    "        config = yaml.load(config_file.read(), Loader=yaml.SafeLoader)\n",
    "\n",
    "    class_labels = config['names']\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "ingest_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da83fbd-2f7e-47f9-9f90-9bb91e3729cc",
   "metadata": {},
   "source": [
    "## Step 2: preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa9ddb-abed-42c9-879c-61c4bab83d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "        data_folder='./data', configuration_path='configuration-local.yaml'):\n",
    "    print('preprocessing data')\n",
    "\n",
    "    for folder in ['images', 'labels']:\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            local_folder = f'{data_folder}/{folder}/{split}'\n",
    "            if not path.exists(local_folder):\n",
    "                makedirs(local_folder)\n",
    "\n",
    "    download_folder = f'{data_folder}/download'\n",
    "    class_labels = _read_class_labels(configuration_path)\n",
    "\n",
    "    folder_names = [class_name.lower() for class_name in class_labels]\n",
    "    images = [\n",
    "        _get_filenames(f'{download_folder}/{folder_name}/images')\n",
    "        for folder_name in folder_names\n",
    "    ]\n",
    "\n",
    "    duplicates_0_1 = images[0] & images[1]\n",
    "    duplicates_1_2 = images[1] & images[2]\n",
    "    duplicates_2_0 = images[2] & images[0]\n",
    "\n",
    "    images[0] -= duplicates_0_1\n",
    "    images[1] -= duplicates_1_2\n",
    "    images[2] -= duplicates_2_0\n",
    "\n",
    "    random.seed(42)\n",
    "    train_ratio = 0.75\n",
    "    val_ratio = 0.125\n",
    "    for i, image_set in enumerate(images):\n",
    "        image_list = list(image_set)\n",
    "        random.shuffle(image_list)\n",
    "        train_size = floor(train_ratio * len(image_list))\n",
    "        val_size = floor(val_ratio * len(image_list))\n",
    "        _split_dataset(\n",
    "            download_folder,\n",
    "            data_folder,\n",
    "            folder_names[i],\n",
    "            image_list,\n",
    "            train_size=train_size,\n",
    "            val_size=val_size,\n",
    "        )\n",
    "\n",
    "    print('data processing done')\n",
    "\n",
    "\n",
    "def _get_filenames(folder):\n",
    "    filenames = set()\n",
    "\n",
    "    for local_path in glob(path.join(folder, '*.jpg')):\n",
    "        filename = path.split(local_path)[-1]\n",
    "        filenames.add(filename)\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "def _split_dataset(\n",
    "        download_folder, data_folder, item, image_names, train_size, val_size):\n",
    "\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        # Label filename\n",
    "        label_name = image_name.replace('.jpg', '.txt')\n",
    "\n",
    "        # Split into train, val, or test\n",
    "        if i < train_size:\n",
    "            split = 'train'\n",
    "        elif i < train_size + val_size:\n",
    "            split = 'val'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "        # Source paths\n",
    "        source_image_path = f'{download_folder}/{item}/images/{image_name}'\n",
    "        source_label_path = f'{download_folder}/{item}/darknet/{label_name}'\n",
    "\n",
    "        # Destination paths\n",
    "        target_image_folder = f'{data_folder}/images/{split}'\n",
    "        target_label_folder = f'{data_folder}/labels/{split}'\n",
    "\n",
    "        # Copy files\n",
    "        copy(source_image_path, target_image_folder)\n",
    "        copy(source_label_path, target_label_folder)\n",
    "\n",
    "\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4343bed-5c1b-4fed-8530-22fb67221ccc",
   "metadata": {},
   "source": [
    "## Step 3: train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54582ac5-9345-4b21-baf7-7f889aa97054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        data_folder='./data', batch_size=0, epochs=0, base_model='yolov5m',\n",
    "        configuration_path='configuration-local.yaml'):\n",
    "    print('training model')\n",
    "\n",
    "    batch_size = batch_size or int(environ.get('batch_size', 4))\n",
    "    epochs = epochs or int(environ.get('epochs', 2))\n",
    "    base_model = base_model or environ.get('base_model', 'yolov5m')\n",
    "\n",
    "    _clean_folder('yolov5/runs')\n",
    "    train.run(\n",
    "        data=configuration_path,\n",
    "        weights=f'{base_model}.pt',\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        freeze=[10],\n",
    "        cache='disk',\n",
    "        exists_ok=True\n",
    "    )\n",
    "\n",
    "    move('yolov5/runs/train/exp/weights/best.pt', 'model.pt')\n",
    "\n",
    "    print('model training done')\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f9f72-7c11-4a26-a74c-7b3d99ea5337",
   "metadata": {},
   "source": [
    "## Step 4: convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3a07a-56aa-4cdf-829f-ded4fc137321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model(model_file_path='model.pt'):\n",
    "    print('converting model')\n",
    "\n",
    "    export.run(\n",
    "        weights=model_file_path,\n",
    "        include=['onnx'],\n",
    "        imgsz=(640, 640),\n",
    "        opset=13,\n",
    "    )\n",
    "\n",
    "    print('model converted')\n",
    "\n",
    "\n",
    "convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db4b9a-76b1-41ff-978c-50a98d5fb3f3",
   "metadata": {},
   "source": [
    "## Step 5: upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe41a83-2c77-4b84-a116-f0dd3b768444",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object_prefix = environ.get('model_object_prefix', 'model')\n",
    "s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')\n",
    "s3_access_key = environ.get('AWS_ACCESS_KEY_ID')\n",
    "s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_bucket_name = environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "\n",
    "def upload_model(model_object_prefix='model', version=''):\n",
    "    s3_client = _initialize_s3_client(\n",
    "        s3_endpoint_url=s3_endpoint_url,\n",
    "        s3_access_key=s3_access_key,\n",
    "        s3_secret_key=s3_secret_key\n",
    "    )\n",
    "    model_object_name = _generate_model_name(\n",
    "        model_object_prefix, version=version\n",
    "    )\n",
    "    _do_upload(s3_client, model_object_name)\n",
    "\n",
    "    model_object_name_latest = _generate_model_name(\n",
    "        model_object_prefix, 'latest'\n",
    "    )\n",
    "    _do_upload(s3_client, model_object_name_latest)\n",
    "\n",
    "\n",
    "def _initialize_s3_client(s3_endpoint_url, s3_access_key, s3_secret_key):\n",
    "    print('initializing S3 client')\n",
    "    s3_client = client(\n",
    "        's3', aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=s3_endpoint_url,\n",
    "    )\n",
    "    return s3_client\n",
    "\n",
    "\n",
    "def _generate_model_name(model_object_prefix, version=''):\n",
    "    version = version if version else _timestamp()\n",
    "    model_name = f'models/{model_object_prefix}-{version}.onnx'\n",
    "    return model_name\n",
    "\n",
    "\n",
    "def _timestamp():\n",
    "    return datetime.now().strftime('%y%m%d%H%M')\n",
    "\n",
    "\n",
    "def _do_upload(s3_client, object_name):\n",
    "    print(f'uploading model to {object_name}')\n",
    "    try:\n",
    "        s3_client.upload_file('model.onnx', s3_bucket_name, object_name)\n",
    "    except:\n",
    "        print(f'S3 upload to bucket {s3_bucket_name} at {s3_endpoint_url} failed!')\n",
    "        raise\n",
    "    print(f'model uploaded and available as \"{object_name}\"')\n",
    "\n",
    "\n",
    "upload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853d621-be3f-4c12-9f19-2ccd7205c874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
